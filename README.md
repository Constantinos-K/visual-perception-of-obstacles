<div align="center">
<h1>Visual Perception of Obstacles: Do Humans and Machines Focus on the Same Image Features?</h1>
<strong>
    Constantinos A. Kyriakides,
    Marios Thoma,
    Zenonas Theodosiou,
    Harris Partaourides,
    Loizos Michael and 
    Andreas Lanitis
</strong>

VISAPP 2024
</div>

![Heatmaps](/visual_perception_of_obstacles/assets/heatmaps.png)

## Abstract

<p align="justify">
Contemporary cities are fractured by a growing number of barriers, such as on-going construction and infrastructure
damages, which endanger pedestrian safety. Automated detection and recognition of such barriers from visual data has
been of particular concern to the research community in recent years. Deep Learning (DL) algorithms are now
the dominant approach in visual data analysis, achieving excellent results in a wide range of applications,
including obstacle detection. However, explaining the underlying operations of DL models remains a key challenge in
gaining significant understanding on how they arrive at their decisions.The use of heatmaps that highlight the focal
points in input images that helped the models reach their predictions has emerged as a form of post-hoc explainability
for such models. In an effort to gain insights into the learning process of DL models, we studied the similarities
between heatmaps generated by a number of DL model architectures trained to detect obstacles on sidewalks in images
collected via smartphones, and eye-tracking heatmaps generated by humans as they detect the corresponding obstacles
on the same data. Our findings indicate that the focus points of humans more closely align with those of a Vision
Transformer architecture, as opposed to the other network architectures we examined in our experiments.
</p>

## Code

(Coming soon.)

The files for computing the quantitative difference between the machine and human heatmaps are the following:
compute_numerical_difference.m
csvMeanArray.m
ImageMeanArray.m
all_comparison_values2.m

Running the script "all_comparison_values2.m" results in the generation of a 20*7 matrix. Each of the rows represents an image from a total subset of 20 images of urban pedestrian obstacles. The columns contain the values for each of the seven Deep Learning models evaluated: "efficientnet_b0", "resnet18", "resnet50", "vgg19", "mobilenet_v2", "swin_b", "vit_b_16". "all_comparison_values2.m" iterates over all pairwise comparisons between each machine heatmap and each human heatmap.

The file "csvMeanArray.m" loads the CSV files containing the machine heatmaps as matrices and divides them into 14\*14 blocks, each containing 16\*16 pixels. In the next step the mean of each single block is computed and stored in a temporary array. The file "ImageMeanArray.m" follows the same procedure as "csvMeanArray.m" applying the same processing steps on the human heatmaps. The only difference is that the human heatmaps are loaded as matrices from PNG images and not CSV files, and they additionally need to be resized to 224*224 to make their dimensions compatible with the machine matrices. Lastly, "compute_numerical_difference.m" receives the array containing the mean block values for the human heatmaps and another array with the same values for the machine heatmaps. The difference between each pair of human-machine array is calculated producing a scalar value of the difference between a particular machine and human heatmap.
